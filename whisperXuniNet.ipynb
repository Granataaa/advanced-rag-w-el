{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34262db8",
   "metadata": {},
   "source": [
    "SPEECH-TO-TEXT\n",
    "\n",
    "whisper_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60d7f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['economia_applicata', 'linguaggio_e_comunicazione']\n",
      "['video\\\\economia_applicata\\\\01_Lez001.mp4', 'video\\\\economia_applicata\\\\02_Lez002.mp4', 'video\\\\economia_applicata\\\\03_Lez003.mp4', 'video\\\\economia_applicata\\\\04_Lez004.mp4', 'video\\\\economia_applicata\\\\05_Lez005.mp4', 'video\\\\economia_applicata\\\\06_Lez006.mp4', 'video\\\\economia_applicata\\\\07_Lez007.mp4', 'video\\\\economia_applicata\\\\08_Lez008.mp4', 'video\\\\economia_applicata\\\\09_Lez009.mp4', 'video\\\\economia_applicata\\\\10_Lez010.mp4', 'video\\\\economia_applicata\\\\11_Lez011.mp4', 'video\\\\economia_applicata\\\\12_Lez012.mp4', 'video\\\\economia_applicata\\\\13_Lez013.mp4', 'video\\\\economia_applicata\\\\14_Lez014.mp4', 'video\\\\economia_applicata\\\\15_Lez015.mp4', 'video\\\\economia_applicata\\\\16_Lez016.mp4', 'video\\\\economia_applicata\\\\17_Lez017.mp4', 'video\\\\economia_applicata\\\\18_Lez018.mp4', 'video\\\\economia_applicata\\\\19_Bianchi_Lez001.mp4', 'video\\\\economia_applicata\\\\20_Bianchi_Lez002.mp4', 'video\\\\economia_applicata\\\\21_Bianchi_Lez003.mp4', 'video\\\\economia_applicata\\\\22_Bianchi_Lez004.mp4', 'video\\\\economia_applicata\\\\23_Bianchi_Lez005.mp4', 'video\\\\economia_applicata\\\\24_Lez006.mp4', 'video\\\\economia_applicata\\\\25_Lez007.mp4', 'video\\\\economia_applicata\\\\26_Lez008.mp4', 'video\\\\economia_applicata\\\\27_Lez009.mp4']\n",
      "video\\economia_applicata\\01_Lez001.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(video)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#trascrizione effettiva del video tramite il modello whisper\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mword_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     37\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#creazione path e nome file output e scrittura su di esso\u001b[39;00m\n\u001b[0;32m     40\u001b[0m nameVideo \u001b[38;5;241m=\u001b[39m video\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\whisper\\transcribe.py:133\u001b[0m, in \u001b[0;36mtranscribe\u001b[1;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[0;32m    130\u001b[0m     decode_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Pad 30-seconds of silence to the input audio, for slicing\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m mel \u001b[38;5;241m=\u001b[39m \u001b[43mlog_mel_spectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_mels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_SAMPLES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m content_frames \u001b[38;5;241m=\u001b[39m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m N_FRAMES\n\u001b[0;32m    135\u001b[0m content_duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(content_frames \u001b[38;5;241m*\u001b[39m HOP_LENGTH \u001b[38;5;241m/\u001b[39m SAMPLE_RATE)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\whisper\\audio.py:148\u001b[0m, in \u001b[0;36mlog_mel_spectrogram\u001b[1;34m(audio, n_mels, padding, device)\u001b[0m\n\u001b[0;32m    146\u001b[0m     audio \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(audio, (\u001b[38;5;241m0\u001b[39m, padding))\n\u001b[0;32m    147\u001b[0m window \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhann_window(N_FFT)\u001b[38;5;241m.\u001b[39mto(audio\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 148\u001b[0m stft \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_FFT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHOP_LENGTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m magnitudes \u001b[38;5;241m=\u001b[39m stft[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mabs() \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    151\u001b[0m filters \u001b[38;5;241m=\u001b[39m mel_filters(audio\u001b[38;5;241m.\u001b[39mdevice, n_mels)\n",
      "File \u001b[1;32mc:\\Users\\franc\\anaconda3\\envs\\whisper_env2\\Lib\\site-packages\\torch\\functional.py:730\u001b[0m, in \u001b[0;36mstft\u001b[1;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex, align_to_window)\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(extended_shape), [pad, pad], pad_mode)\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39msignal_dim:])\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43malign_to_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "#controlla se c'Ã¨ la GPU e gli carica il modello\n",
    "assert torch.cuda.is_available(), \"GPU non rilevata!\"\n",
    "model = whisper.load_model(\"turbo\").to(\"cuda\")\n",
    "\n",
    "\n",
    "#prende cartelle e file per input e output\n",
    "instance_folder = config['directoryVideo']['path']\n",
    "folder_transcription = \"./transcription/\"\n",
    "\n",
    "folders = [name for name in os.listdir(instance_folder) if os.path.isdir(os.path.join(instance_folder, name))]\n",
    "print(folders)\n",
    "\n",
    "for folder in folders:\n",
    "    folder_transcription = os.path.join(folder_transcription, folder)\n",
    "    \n",
    "    path = os.path.join(instance_folder, folder)\n",
    "    instance_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".mp4\")]\n",
    "    print(instance_files)\n",
    "\n",
    "    #for principale\n",
    "    for video in instance_files:\n",
    "        print(video)\n",
    "        #trascrizione effettiva del video tramite il modello whisper\n",
    "        result = model.transcribe(\n",
    "            video,                  \n",
    "            language=\"it\",          \n",
    "            word_timestamps=True,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        #creazione path e nome file output e scrittura su di esso\n",
    "        nameVideo = video.split(\"\\\\\")\n",
    "        nameFile = nameVideo[len(nameVideo)-1]\n",
    "        nameFile = nameFile.split(\".\")[0]\n",
    "        nameFile = folder_transcription + nameFile + \".txt\"\n",
    "        print(nameFile)\n",
    "        with open(nameFile, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(result['text'] + \"\\n\\n\")\n",
    "            f.write(\"PAROLA\".ljust(20) + \"INIZIO\".ljust(15) + \"FINE\".ljust(15) + \"\\n\")\n",
    "            f.write(\"-\"*50 + \"\\n\")\n",
    "            \n",
    "            for segment in result[\"segments\"]:\n",
    "                if \"words\" in segment:\n",
    "                    for word in segment[\"words\"]:\n",
    "                        line = (word['word'].ljust(20) + \n",
    "                            f\"{word['start']:.2f}s\".ljust(15) + \n",
    "                            f\"{word['end']:.2f}s\".ljust(15))\n",
    "                        f.write(line + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
