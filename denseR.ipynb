{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b6bc52e",
   "metadata": {},
   "source": [
    "DENSE RETRIEVAL - DA TESTO A EMBEDDINGS - CALCOLO FAISS\n",
    "\n",
    "whisper_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49bb73fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "import json\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd8e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('intfloat/multilingual-e5-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55be068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il modello linguistico italiano di spaCy\n",
    "nlp = spacy.load(\"it_core_news_lg\")\n",
    "\n",
    "def chunk_text(text, max_tokens=300, min_tokens=20):\n",
    "    \"\"\"\n",
    "    Suddivide un testo in chunk (blocchi) di dimensioni specificate, rispettando i confini delle frasi.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Il testo da suddividere\n",
    "        max_tokens (int, optional): Numero massimo di token per chunk. Default: 300\n",
    "        min_tokens (int, optional): Numero minimo di token per chunk. Default: 20\n",
    "    \n",
    "    Returns:\n",
    "        list: Una lista di chunk di testo\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_len = 0\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        token_len = len(sent.text.split())\n",
    "        \n",
    "        if current_len + token_len <= max_tokens:\n",
    "            current_chunk.append(sent.text)\n",
    "            current_len += token_len\n",
    "        else:\n",
    "            # Solo aggiungi il chunk se ha almeno min_tokens\n",
    "            chunk_text_str = \" \".join(current_chunk)\n",
    "            if len(chunk_text_str.split()) >= min_tokens:\n",
    "                chunks.append(chunk_text_str)\n",
    "            current_chunk = [sent.text]\n",
    "            current_len = token_len\n",
    "\n",
    "    # Aggiungi l'ultimo chunk se è sufficientemente lungo\n",
    "    if current_chunk:\n",
    "        chunk_text_str = \" \".join(current_chunk)\n",
    "        if len(chunk_text_str.split()) >= min_tokens:\n",
    "            chunks.append(chunk_text_str)\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e9ce79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa SequenceMatcher per il confronto tra stringhe\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def normalized(word):\n",
    "    \"\"\"\n",
    "    Normalizza una parola rimuovendo punteggiatura e convertendo in minuscolo.\n",
    "    \n",
    "    Args:\n",
    "        word (str): Parola da normalizzare\n",
    "    \n",
    "    Returns:\n",
    "        str: Parola normalizzata\n",
    "    \"\"\"\n",
    "    return word.strip(\".,?!;:\").lower()\n",
    "\n",
    "def similarity(a, b):\n",
    "    \"\"\"\n",
    "    Calcola la similarità tra due stringhe usando il rapporto di SequenceMatcher.\n",
    "    \n",
    "    Args:\n",
    "        a (str): Prima stringa da confrontare\n",
    "        b (str): Seconda stringa da confrontare\n",
    "    \n",
    "    Returns:\n",
    "        float: Punteggio di similarità (0.0-1.0)\n",
    "    \"\"\"\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def match_chunk_with_timestamps_fuzzy(chunk_text, word_list, prev, tolerance=0.50):\n",
    "    \"\"\"\n",
    "    Trova il miglior match tra un chunk di testo e una lista di parole con timestamp,\n",
    "    usando un approccio fuzzy (approssimato).\n",
    "    \n",
    "    Args:\n",
    "        chunk_text (str): Testo da abbinare\n",
    "        word_list (list): Lista di dizionari con parole e timestamp (formato: {\"text\": \"...\", \"start\": x, \"end\": y})\n",
    "        prev (float): Timestamp precedente (per evitare sovrapposizioni)\n",
    "        tolerance (float, optional): Soglia minima di similarità. Default: 0.50\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (start_time, end_time) oppure (None, None) se nessun match trovato\n",
    "    \"\"\"\n",
    "    print(\"fuzzy\")\n",
    "\n",
    "    # Normalizza le parole del chunk\n",
    "    chunk_words = [normalized(w) for w in chunk_text.split()]\n",
    "\n",
    "    start_time = None\n",
    "    \n",
    "    # Gestione caso speciale: chunk vuoto\n",
    "    if not chunk_words:\n",
    "        return None, None\n",
    "\n",
    "    # Normalizza le parole della trascrizione\n",
    "    transcript_words = [normalized(w[\"text\"]) for w in word_list]\n",
    "\n",
    "    max_score = 0  # Memorizza il punteggio di similarità massimo trovato\n",
    "    best_match = None  # Memorizza l'indice del miglior match\n",
    "\n",
    "    # Scorri la trascrizione con una finestra mobile della stessa lunghezza del chunk\n",
    "    for i in range(len(transcript_words) - len(chunk_words) + 1):\n",
    "        # Estrai la finestra corrente\n",
    "        window = transcript_words[i:i + len(chunk_words)]\n",
    "        \n",
    "        # Calcola il punteggio medio di similarità per questa finestra\n",
    "        score = sum(similarity(w1, w2) for w1, w2 in zip(chunk_words, window)) / len(chunk_words)\n",
    "\n",
    "        # Aggiorna il miglior match se trovato un punteggio più alto\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            best_match = i\n",
    "\n",
    "    # Se il punteggio massimo supera la tolleranza, restituisci i timestamp\n",
    "    if max_score >= tolerance:\n",
    "        start_time = word_list[best_match][\"start\"]\n",
    "        end_time = word_list[best_match + len(chunk_words) - 1][\"end\"]\n",
    "        \n",
    "        return start_time, end_time \n",
    "    \n",
    "    print(\"Chunk senza match:\", chunk_words[:200])  # Prime 200 chars del chunk problematico\n",
    "    print(\"Trascrizione corrispondente:\", [w[\"text\"] for w in word_list][:20])  # Prime 20 parole della trascrizione\n",
    "\n",
    "    # Gestione dei casi particolari per start_time\n",
    "    if start_time == None:\n",
    "        start_time = prev\n",
    "    elif start_time < prev:\n",
    "        start_time = prev\n",
    "\n",
    "    # Nessun match sufficientemente buono trovato\n",
    "    return start_time, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bbad4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_chunk_exact(chunk_text, word_list, prev, lookahead=10):\n",
    "    \"\"\"\n",
    "    Cerca il match esatto delle prime N parole del chunk nella trascrizione.\n",
    "    \n",
    "    Args:\n",
    "        chunk_text (str): Testo del chunk da abbinare\n",
    "        word_list (list): Lista di dizionari delle parole con timestamp\n",
    "        prev (float): Ultimo timestamp valido conosciuto\n",
    "        lookahead (int): Numero di parole da usare per il matching iniziale\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (start_time, end_time) o (None, None) se non trovato\n",
    "    \"\"\"\n",
    "    # Prendi le prime 'lookahead' parole del chunk (normalizzate)\n",
    "    chunk_words = [normalized(w) for w in chunk_text.split()[:lookahead] if w.strip()]\n",
    "    start_time = None\n",
    "    \n",
    "    if not chunk_words:\n",
    "        return None, None\n",
    "    \n",
    "    # Crea una lista di parole normalizzate dalla trascrizione\n",
    "    transcript_words = [normalized(w[\"text\"]) for w in word_list]\n",
    "    \n",
    "    # Cerca la posizione di inizio del match\n",
    "    for i in range(len(transcript_words) - len(chunk_words) + 1):\n",
    "        match = True\n",
    "        for j in range(len(chunk_words)):\n",
    "            if transcript_words[i+j] != chunk_words[j]:\n",
    "                match = False\n",
    "                break\n",
    "        \n",
    "        if match:\n",
    "            start_idx = i\n",
    "            end_idx = i + len(chunk_text.split()) - 1\n",
    "            \n",
    "            # Controllo sicurezza sugli indici\n",
    "            if end_idx >= len(word_list):\n",
    "                end_idx = len(word_list) - 1\n",
    "                \n",
    "            start_time = word_list[start_idx][\"start\"]\n",
    "            end_time = word_list[end_idx][\"end\"]\n",
    "            \n",
    "            \n",
    "                \n",
    "            return start_time, end_time\n",
    "    # Gestione casi particolari\n",
    "    if start_time is None:\n",
    "        start_time = prev\n",
    "    elif start_time < prev:\n",
    "        start_time = prev\n",
    "\n",
    "    return start_time, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed55f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_chunk_hybrid(chunk_text, word_list, prev):\n",
    "    # Prima prova matching esatto sulle prime 3 parole\n",
    "    exact_match = match_chunk_exact(chunk_text, word_list, prev)\n",
    "    if exact_match != (None, None):\n",
    "        return exact_match\n",
    "    \n",
    "    # Fallback al fuzzy solo se necessario\n",
    "    return match_chunk_with_timestamps_fuzzy(chunk_text, word_list, prev, tolerance=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "094220af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "economia_applicata_clean_01_Lez001.txt\n",
      "0 / 12\n",
      "1 / 12\n",
      "2 / 12\n",
      "3 / 12\n",
      "4 / 12\n",
      "5 / 12\n",
      "6 / 12\n",
      "7 / 12\n",
      "8 / 12\n",
      "9 / 12\n",
      "10 / 12\n",
      "11 / 12\n",
      "economia_applicata_clean_02_Lez002.txt\n",
      "0 / 17\n",
      "1 / 17\n",
      "2 / 17\n",
      "3 / 17\n",
      "4 / 17\n",
      "5 / 17\n",
      "6 / 17\n",
      "7 / 17\n",
      "8 / 17\n",
      "9 / 17\n",
      "10 / 17\n",
      "11 / 17\n",
      "12 / 17\n",
      "13 / 17\n",
      "14 / 17\n",
      "15 / 17\n",
      "16 / 17\n",
      "economia_applicata_clean_03_Lez003.txt\n",
      "0 / 9\n",
      "1 / 9\n",
      "2 / 9\n",
      "3 / 9\n",
      "4 / 9\n",
      "5 / 9\n",
      "6 / 9\n",
      "7 / 9\n",
      "8 / 9\n",
      "economia_applicata_clean_04_Lez004.txt\n",
      "0 / 10\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "economia_applicata_clean_05_Lez005.txt\n",
      "0 / 13\n",
      "1 / 13\n",
      "2 / 13\n",
      "3 / 13\n",
      "4 / 13\n",
      "5 / 13\n",
      "6 / 13\n",
      "7 / 13\n",
      "8 / 13\n",
      "9 / 13\n",
      "10 / 13\n",
      "11 / 13\n",
      "12 / 13\n",
      "economia_applicata_clean_06_Lez006.txt\n",
      "0 / 11\n",
      "1 / 11\n",
      "2 / 11\n",
      "3 / 11\n",
      "4 / 11\n",
      "5 / 11\n",
      "6 / 11\n",
      "7 / 11\n",
      "8 / 11\n",
      "9 / 11\n",
      "10 / 11\n",
      "economia_applicata_clean_07_Lez007.txt\n",
      "0 / 16\n",
      "1 / 16\n",
      "2 / 16\n",
      "3 / 16\n",
      "4 / 16\n",
      "5 / 16\n",
      "6 / 16\n",
      "7 / 16\n",
      "8 / 16\n",
      "9 / 16\n",
      "10 / 16\n",
      "11 / 16\n",
      "12 / 16\n",
      "13 / 16\n",
      "14 / 16\n",
      "15 / 16\n",
      "economia_applicata_clean_08_Lez008.txt\n",
      "0 / 16\n",
      "1 / 16\n",
      "2 / 16\n",
      "3 / 16\n",
      "4 / 16\n",
      "5 / 16\n",
      "6 / 16\n",
      "7 / 16\n",
      "8 / 16\n",
      "9 / 16\n",
      "10 / 16\n",
      "11 / 16\n",
      "12 / 16\n",
      "13 / 16\n",
      "14 / 16\n",
      "15 / 16\n",
      "economia_applicata_clean_09_Lez009.txt\n",
      "0 / 10\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "economia_applicata_clean_10_Lez010.txt\n",
      "0 / 13\n",
      "1 / 13\n",
      "2 / 13\n",
      "3 / 13\n",
      "4 / 13\n",
      "5 / 13\n",
      "6 / 13\n",
      "7 / 13\n",
      "8 / 13\n",
      "9 / 13\n",
      "10 / 13\n",
      "11 / 13\n",
      "12 / 13\n",
      "economia_applicata_clean_11_Lez011.txt\n",
      "0 / 10\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "economia_applicata_clean_12_Lez012.txt\n",
      "0 / 8\n",
      "1 / 8\n",
      "2 / 8\n",
      "3 / 8\n",
      "4 / 8\n",
      "5 / 8\n",
      "6 / 8\n",
      "7 / 8\n",
      "economia_applicata_clean_13_Lez013.txt\n",
      "0 / 15\n",
      "1 / 15\n",
      "2 / 15\n",
      "3 / 15\n",
      "4 / 15\n",
      "5 / 15\n",
      "6 / 15\n",
      "7 / 15\n",
      "8 / 15\n",
      "9 / 15\n",
      "10 / 15\n",
      "11 / 15\n",
      "12 / 15\n",
      "13 / 15\n",
      "14 / 15\n",
      "economia_applicata_clean_14_Lez014.txt\n",
      "0 / 18\n",
      "1 / 18\n",
      "2 / 18\n",
      "3 / 18\n",
      "4 / 18\n",
      "5 / 18\n",
      "6 / 18\n",
      "7 / 18\n",
      "8 / 18\n",
      "9 / 18\n",
      "10 / 18\n",
      "11 / 18\n",
      "12 / 18\n",
      "13 / 18\n",
      "14 / 18\n",
      "15 / 18\n",
      "16 / 18\n",
      "17 / 18\n",
      "economia_applicata_clean_15_Lez015.txt\n",
      "0 / 7\n",
      "1 / 7\n",
      "2 / 7\n",
      "3 / 7\n",
      "4 / 7\n",
      "5 / 7\n",
      "6 / 7\n",
      "economia_applicata_clean_16_Lez016.txt\n",
      "0 / 9\n",
      "1 / 9\n",
      "2 / 9\n",
      "3 / 9\n",
      "4 / 9\n",
      "5 / 9\n",
      "6 / 9\n",
      "7 / 9\n",
      "8 / 9\n",
      "economia_applicata_clean_17_Lez017.txt\n",
      "0 / 17\n",
      "1 / 17\n",
      "2 / 17\n",
      "3 / 17\n",
      "4 / 17\n",
      "5 / 17\n",
      "6 / 17\n",
      "7 / 17\n",
      "8 / 17\n",
      "9 / 17\n",
      "10 / 17\n",
      "11 / 17\n",
      "12 / 17\n",
      "13 / 17\n",
      "14 / 17\n",
      "15 / 17\n",
      "16 / 17\n",
      "economia_applicata_clean_18_Lez018.txt\n",
      "0 / 13\n",
      "1 / 13\n",
      "2 / 13\n",
      "3 / 13\n",
      "4 / 13\n",
      "5 / 13\n",
      "6 / 13\n",
      "7 / 13\n",
      "8 / 13\n",
      "9 / 13\n",
      "10 / 13\n",
      "11 / 13\n",
      "12 / 13\n",
      "economia_applicata_clean_19_Bianchi_Lez001.txt\n",
      "0 / 12\n",
      "1 / 12\n",
      "2 / 12\n",
      "3 / 12\n",
      "4 / 12\n",
      "5 / 12\n",
      "6 / 12\n",
      "7 / 12\n",
      "8 / 12\n",
      "9 / 12\n",
      "10 / 12\n",
      "11 / 12\n",
      "economia_applicata_clean_20_Bianchi_Lez002.txt\n",
      "0 / 18\n",
      "1 / 18\n",
      "2 / 18\n",
      "3 / 18\n",
      "4 / 18\n",
      "5 / 18\n",
      "6 / 18\n",
      "7 / 18\n",
      "8 / 18\n",
      "9 / 18\n",
      "10 / 18\n",
      "11 / 18\n",
      "12 / 18\n",
      "13 / 18\n",
      "14 / 18\n",
      "15 / 18\n",
      "16 / 18\n",
      "17 / 18\n",
      "economia_applicata_clean_21_Bianchi_Lez003.txt\n",
      "0 / 17\n",
      "1 / 17\n",
      "2 / 17\n",
      "3 / 17\n",
      "4 / 17\n",
      "5 / 17\n",
      "6 / 17\n",
      "7 / 17\n",
      "8 / 17\n",
      "9 / 17\n",
      "10 / 17\n",
      "11 / 17\n",
      "12 / 17\n",
      "13 / 17\n",
      "14 / 17\n",
      "15 / 17\n",
      "16 / 17\n",
      "economia_applicata_clean_22_Bianchi_Lez004.txt\n",
      "0 / 7\n",
      "1 / 7\n",
      "2 / 7\n",
      "3 / 7\n",
      "4 / 7\n",
      "5 / 7\n",
      "6 / 7\n",
      "economia_applicata_clean_23_Bianchi_Lez005.txt\n",
      "0 / 15\n",
      "1 / 15\n",
      "2 / 15\n",
      "3 / 15\n",
      "4 / 15\n",
      "5 / 15\n",
      "6 / 15\n",
      "7 / 15\n",
      "8 / 15\n",
      "9 / 15\n",
      "10 / 15\n",
      "11 / 15\n",
      "12 / 15\n",
      "13 / 15\n",
      "14 / 15\n",
      "economia_applicata_clean_24_Lez006.txt\n",
      "0 / 10\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "economia_applicata_clean_25_Lez007.txt\n",
      "0 / 18\n",
      "1 / 18\n",
      "2 / 18\n",
      "3 / 18\n",
      "4 / 18\n",
      "5 / 18\n",
      "6 / 18\n",
      "7 / 18\n",
      "8 / 18\n",
      "9 / 18\n",
      "10 / 18\n",
      "11 / 18\n",
      "12 / 18\n",
      "13 / 18\n",
      "14 / 18\n",
      "15 / 18\n",
      "16 / 18\n",
      "17 / 18\n",
      "economia_applicata_clean_26_Lez008.txt\n",
      "0 / 14\n",
      "1 / 14\n",
      "2 / 14\n",
      "3 / 14\n",
      "4 / 14\n",
      "5 / 14\n",
      "6 / 14\n",
      "7 / 14\n",
      "8 / 14\n",
      "9 / 14\n",
      "10 / 14\n",
      "11 / 14\n",
      "12 / 14\n",
      "13 / 14\n",
      "economia_applicata_clean_27_Lez009.txt\n",
      "0 / 16\n",
      "1 / 16\n",
      "2 / 16\n",
      "3 / 16\n",
      "4 / 16\n",
      "5 / 16\n",
      "6 / 16\n",
      "7 / 16\n",
      "8 / 16\n",
      "9 / 16\n",
      "10 / 16\n",
      "11 / 16\n",
      "12 / 16\n",
      "13 / 16\n",
      "14 / 16\n",
      "15 / 16\n",
      "linguaggio_e_comunicazione_clean_Lez001.txt\n",
      "0 / 15\n",
      "1 / 15\n",
      "2 / 15\n",
      "3 / 15\n",
      "4 / 15\n",
      "5 / 15\n",
      "6 / 15\n",
      "7 / 15\n",
      "8 / 15\n",
      "9 / 15\n",
      "10 / 15\n",
      "11 / 15\n",
      "12 / 15\n",
      "13 / 15\n",
      "14 / 15\n",
      "linguaggio_e_comunicazione_clean_Lez001Medium.txt\n",
      "0 / 18\n",
      "1 / 18\n",
      "2 / 18\n",
      "3 / 18\n",
      "4 / 18\n",
      "5 / 18\n",
      "6 / 18\n",
      "7 / 18\n",
      "8 / 18\n",
      "9 / 18\n",
      "10 / 18\n",
      "11 / 18\n",
      "12 / 18\n",
      "13 / 18\n",
      "14 / 18\n",
      "15 / 18\n",
      "16 / 18\n",
      "17 / 18\n",
      "linguaggio_e_comunicazione_clean_Lez001Small.txt\n",
      "0 / 18\n",
      "1 / 18\n",
      "2 / 18\n",
      "3 / 18\n",
      "4 / 18\n",
      "5 / 18\n",
      "6 / 18\n",
      "7 / 18\n",
      "8 / 18\n",
      "9 / 18\n",
      "10 / 18\n",
      "11 / 18\n",
      "12 / 18\n",
      "13 / 18\n",
      "14 / 18\n",
      "15 / 18\n",
      "16 / 18\n",
      "17 / 18\n",
      "linguaggio_e_comunicazione_clean_Lez002.txt\n",
      "0 / 16\n",
      "1 / 16\n",
      "2 / 16\n",
      "3 / 16\n",
      "4 / 16\n",
      "5 / 16\n",
      "6 / 16\n",
      "7 / 16\n",
      "8 / 16\n",
      "9 / 16\n",
      "10 / 16\n",
      "11 / 16\n",
      "12 / 16\n",
      "13 / 16\n",
      "14 / 16\n",
      "15 / 16\n",
      "linguaggio_e_comunicazione_clean_Lez003.txt\n",
      "0 / 17\n",
      "1 / 17\n",
      "2 / 17\n",
      "3 / 17\n",
      "4 / 17\n",
      "5 / 17\n",
      "6 / 17\n",
      "7 / 17\n",
      "8 / 17\n",
      "9 / 17\n",
      "10 / 17\n",
      "11 / 17\n",
      "12 / 17\n",
      "13 / 17\n",
      "14 / 17\n",
      "15 / 17\n",
      "16 / 17\n",
      "linguaggio_e_comunicazione_clean_Lez004.txt\n",
      "0 / 18\n",
      "1 / 18\n",
      "2 / 18\n",
      "3 / 18\n",
      "4 / 18\n",
      "5 / 18\n",
      "6 / 18\n",
      "7 / 18\n",
      "8 / 18\n",
      "9 / 18\n",
      "10 / 18\n",
      "11 / 18\n",
      "12 / 18\n",
      "13 / 18\n",
      "14 / 18\n",
      "15 / 18\n",
      "16 / 18\n",
      "17 / 18\n",
      "linguaggio_e_comunicazione_clean_Lez005.txt\n",
      "0 / 13\n",
      "1 / 13\n",
      "2 / 13\n",
      "3 / 13\n",
      "4 / 13\n",
      "5 / 13\n",
      "6 / 13\n",
      "7 / 13\n",
      "8 / 13\n",
      "9 / 13\n",
      "10 / 13\n",
      "11 / 13\n",
      "12 / 13\n",
      "linguaggio_e_comunicazione_clean_Lez006.txt\n",
      "0 / 15\n",
      "1 / 15\n",
      "2 / 15\n",
      "3 / 15\n",
      "4 / 15\n",
      "5 / 15\n",
      "6 / 15\n",
      "7 / 15\n",
      "8 / 15\n",
      "9 / 15\n",
      "10 / 15\n",
      "11 / 15\n",
      "12 / 15\n",
      "13 / 15\n",
      "14 / 15\n",
      "linguaggio_e_comunicazione_clean_Lez007.txt\n",
      "0 / 15\n",
      "1 / 15\n",
      "2 / 15\n",
      "3 / 15\n",
      "4 / 15\n",
      "5 / 15\n",
      "6 / 15\n",
      "7 / 15\n",
      "8 / 15\n",
      "9 / 15\n",
      "10 / 15\n",
      "11 / 15\n",
      "12 / 15\n",
      "13 / 15\n",
      "14 / 15\n",
      "linguaggio_e_comunicazione_clean_Lez008.txt\n",
      "0 / 17\n",
      "1 / 17\n",
      "2 / 17\n",
      "3 / 17\n",
      "4 / 17\n",
      "5 / 17\n",
      "6 / 17\n",
      "7 / 17\n",
      "8 / 17\n",
      "9 / 17\n",
      "10 / 17\n",
      "11 / 17\n",
      "12 / 17\n",
      "13 / 17\n",
      "14 / 17\n",
      "15 / 17\n",
      "16 / 17\n",
      "linguaggio_e_comunicazione_clean_Lez009.txt\n",
      "0 / 10\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "linguaggio_e_comunicazione_clean_Lez010.txt\n",
      "0 / 11\n",
      "1 / 11\n",
      "2 / 11\n",
      "3 / 11\n",
      "4 / 11\n",
      "5 / 11\n",
      "6 / 11\n",
      "7 / 11\n",
      "8 / 11\n",
      "9 / 11\n",
      "10 / 11\n",
      "linguaggio_e_comunicazione_clean_Lez011.txt\n",
      "0 / 14\n",
      "1 / 14\n",
      "2 / 14\n",
      "3 / 14\n",
      "4 / 14\n",
      "5 / 14\n",
      "6 / 14\n",
      "7 / 14\n",
      "8 / 14\n",
      "9 / 14\n",
      "10 / 14\n",
      "11 / 14\n",
      "12 / 14\n",
      "13 / 14\n",
      "linguaggio_e_comunicazione_clean_Lez012.txt\n",
      "0 / 20\n",
      "1 / 20\n",
      "2 / 20\n",
      "3 / 20\n",
      "4 / 20\n",
      "5 / 20\n",
      "6 / 20\n",
      "7 / 20\n",
      "8 / 20\n",
      "9 / 20\n",
      "10 / 20\n",
      "11 / 20\n",
      "12 / 20\n",
      "13 / 20\n",
      "14 / 20\n",
      "15 / 20\n",
      "16 / 20\n",
      "17 / 20\n",
      "18 / 20\n",
      "19 / 20\n",
      "linguaggio_e_comunicazione_clean_Lez013.txt\n",
      "0 / 12\n",
      "1 / 12\n",
      "2 / 12\n",
      "3 / 12\n",
      "4 / 12\n",
      "5 / 12\n",
      "6 / 12\n",
      "7 / 12\n",
      "8 / 12\n",
      "9 / 12\n",
      "10 / 12\n",
      "11 / 12\n",
      "linguaggio_e_comunicazione_clean_Lez014.txt\n",
      "0 / 13\n",
      "1 / 13\n",
      "2 / 13\n",
      "3 / 13\n",
      "4 / 13\n",
      "5 / 13\n",
      "6 / 13\n",
      "7 / 13\n",
      "8 / 13\n",
      "9 / 13\n",
      "10 / 13\n",
      "11 / 13\n",
      "12 / 13\n",
      "linguaggio_e_comunicazione_clean_Lez015.txt\n",
      "0 / 12\n",
      "1 / 12\n",
      "2 / 12\n",
      "3 / 12\n",
      "4 / 12\n",
      "5 / 12\n",
      "6 / 12\n",
      "7 / 12\n",
      "8 / 12\n",
      "9 / 12\n",
      "10 / 12\n",
      "11 / 12\n",
      "linguaggio_e_comunicazione_clean_Volterra_Lez001.txt\n",
      "0 / 11\n",
      "1 / 11\n",
      "2 / 11\n",
      "3 / 11\n",
      "4 / 11\n",
      "5 / 11\n",
      "6 / 11\n",
      "7 / 11\n",
      "8 / 11\n",
      "9 / 11\n",
      "10 / 11\n",
      "linguaggio_e_comunicazione_clean_Volterra_Lez002.txt\n",
      "0 / 14\n",
      "1 / 14\n",
      "2 / 14\n",
      "3 / 14\n",
      "4 / 14\n",
      "5 / 14\n",
      "6 / 14\n",
      "7 / 14\n",
      "8 / 14\n",
      "9 / 14\n",
      "10 / 14\n",
      "11 / 14\n",
      "12 / 14\n",
      "13 / 14\n",
      "linguaggio_e_comunicazione_clean_Volterra_Lez003.txt\n",
      "0 / 14\n",
      "1 / 14\n",
      "2 / 14\n",
      "3 / 14\n",
      "4 / 14\n",
      "5 / 14\n",
      "6 / 14\n",
      "7 / 14\n",
      "8 / 14\n",
      "9 / 14\n",
      "10 / 14\n",
      "11 / 14\n",
      "12 / 14\n",
      "13 / 14\n",
      "linguaggio_e_comunicazione_clean_Volterra_Lez004.txt\n",
      "0 / 10\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "linguaggio_e_comunicazione_clean_Volterra_Lez005.txt\n",
      "0 / 13\n",
      "1 / 13\n",
      "2 / 13\n",
      "3 / 13\n",
      "4 / 13\n",
      "5 / 13\n",
      "6 / 13\n",
      "7 / 13\n",
      "8 / 13\n",
      "9 / 13\n",
      "10 / 13\n",
      "11 / 13\n",
      "12 / 13\n",
      "linguaggio_e_comunicazione_clean_Volterra_Lez006.txt\n",
      "0 / 9\n",
      "1 / 9\n",
      "2 / 9\n",
      "3 / 9\n",
      "4 / 9\n",
      "5 / 9\n",
      "6 / 9\n",
      "7 / 9\n",
      "8 / 9\n"
     ]
    }
   ],
   "source": [
    "# Inizializzazione delle liste per memorizzare i chunk e i metadati associati\n",
    "all_chunks = []      # Contiene il testo dei chunk\n",
    "all_chunk_data = []  # Contiene dizionari con metadati e informazioni sui chunk\n",
    "prev = 0             # Tiene traccia del timestamp finale dell'ultimo chunk processato\n",
    "\n",
    "# Directory contenente i file di testo da elaborare\n",
    "input_dir = \"datasetOnlyTextUni\\\\onlyTextLessonsTurbo\"\n",
    "\n",
    "# Itera attraverso tutti i file nella directory di input\n",
    "for filename in os.listdir(input_dir):\n",
    "    print(filename)  # Stampa il nome del file corrente per tracciare l'avanzamento\n",
    "    \n",
    "    # Processa solo i file con estensione .txt\n",
    "    if filename.endswith(\".txt\"):\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "\n",
    "        # Legge il contenuto del file di testo\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            long_text = f.read()\n",
    "\n",
    "        # Divide il testo lungo in chunk utilizzando la funzione chunk_text\n",
    "        chunks = chunk_text(long_text)\n",
    "        all_chunks.extend(chunks)  # Aggiunge i chunk alla lista globale\n",
    "        \n",
    "        # Carica il file JSON con i timestamp corrispondente al file di testo\n",
    "        with open(f\"datasetOnlyTextUni\\\\timestamp_json_turbo\\\\{filename.replace(\".txt\", \".json\")}\", \n",
    "                 \"r\", encoding=\"utf-8\") as f:\n",
    "            words = json.load(f)  # Carica i dati JSON contenenti parole e timestamp\n",
    "        \n",
    "        # Itera attraverso tutti i chunk creati dal file corrente\n",
    "        for i, c in enumerate(chunks):\n",
    "            print(f\"{i} / {len(chunks)}\")  # Stampa progresso elaborazione chunk\n",
    "            \n",
    "            # Trova i timestamp di inizio e fine per il chunk corrente\n",
    "            start, end = match_chunk_hybrid(c, words, prev)\n",
    "            \n",
    "            # Aggiorna il timestamp precedente per il prossimo chunk\n",
    "            if end != None:\n",
    "                prev = end\n",
    "            else:\n",
    "                prev = 0  # Resetta se non è stato trovato un match\n",
    "                \n",
    "            # Estrae le entità nominate dal chunk usando spaCy\n",
    "            entities = [{\"text\": ent.text, \"label\": ent.label_} for ent in nlp(c).ents]\n",
    "\n",
    "            # Entity linking con relik\n",
    "            #linked_entities = link_entities(c, chunk_id=i)\n",
    "\n",
    "            # Aggiunge tutte le informazioni alla lista dei metadati\n",
    "            all_chunk_data.append({\n",
    "                \"text\": c,            # Testo del chunk\n",
    "                \"source\": filename,    # Nome del file di origine\n",
    "                \"chunk_id\": i,        # Indice del chunk nel file\n",
    "                \"start_time\": start,   # Timestamp di inizio (se trovato)\n",
    "                \"end_time\": end,      # Timestamp di fine (se trovato)\n",
    "                \"entities\": entities  # Lista di entità nominate trovate\n",
    "                #\"kb_links\": linked_entities  # Aggiungi questo campo\n",
    "            })\n",
    "\n",
    "# Genera gli embeddings per tutti i chunk usando il modello\n",
    "embeddings = model.encode(all_chunks, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4204f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cercando di riempire timestamp vuoti\n",
    "\n",
    "for i, c in enumerate(all_chunk_data):\n",
    "    if c[\"start_time\"] is None and c[\"end_time\"] is None:\n",
    "        # Usa il fine del chunk precedente come inizio, se disponibile\n",
    "        if i > 0 and all_chunk_data[i - 1][\"end_time\"] is not None:\n",
    "            c[\"start_time\"] = all_chunk_data[i - 1][\"end_time\"]\n",
    "\n",
    "        # Usa l'inizio del chunk successivo come fine, se disponibile\n",
    "        if i < len(all_chunk_data) - 1 and all_chunk_data[i + 1][\"start_time\"] is not None:\n",
    "            c[\"end_time\"] = all_chunk_data[i + 1][\"start_time\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648e8ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indice FAISS salvato!\n",
      "✅ Embeddings e metadati salvati!\n"
     ]
    }
   ],
   "source": [
    "# Salva gli embeddings (come float32 per FAISS)\n",
    "np.save(\"embeddings300.npy\", embeddings.astype(\"float32\"))\n",
    "\n",
    "# Crea l'indice FAISS (cosine similarity: inner product su embeddings normalizzati)\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "# Salva l'indice\n",
    "faiss.write_index(index, \"faiss_index300.index\")\n",
    "print(\"✅ Indice FAISS salvato!\")\n",
    "\n",
    "# Salva i metadati\n",
    "with open(\"chunks_metadata300.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunk_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ Embeddings e metadati salvati!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
